# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


#
# This file defines the Flink build for the "apache/flink" repository, including
# the following:
#  - PR builds (triggered through ci-bot)
#  - custom triggered e2e tests
#  - nightly builds

name: "Hadoop 2.8.3 (Nico)"

on:
  push:
    branches:
      - hackathon2021b-nico
  workflow_dispatch:
    inputs:
      run_end_to_end:
        description: "If enabled, the end to end tests will be executed"
        required: false
      e2e_group:
        description: "The group of e2e tests that should run. Can be 1 or 2"
        required: false


env:
  STAGE_NAME: "ci"
  ENVIRONMENT: 'PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12"'
  JDK: 8
  MODE: ""


jobs:
#  compile:
#    uses: ververica/flink/.github/workflows/compile-template.yml@hackathon2021b-nico
#    with:
#      stage_name: "ci"
#      environment: 'PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12"'
#      jdk: 8
#      mode: ""
#
#  test:
#    uses: ververica/flink/.github/workflows/test-template.yml@hackathon2021b-nico
#    needs: compile
#    with:
#      stage_name: "ci"
#      environment: 'PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12"'
#      jdk: 8
#      mode: ""
#    secrets:
#      s3_bucket: ${{ secrets.s3_bucket }}
#      s3_access_key: ${{ secrets.s3_access_key }}
#      s3_secret_key: ${{ secrets.s3_secret_key }}
#      glue_schema_access_key: ${{ secrets.glue_schema_access_key }}
#      glue_schema_secret_key: ${{ secrets.glue_schema_secret_key }}

  e2e-test:
    uses: ververica/flink/.github/workflows/e2e-template.yml@hackathon2021b-nico
#    if: ${{ github.event.inputs.run_end_to_end == 'true' }}
    with:
      stage_name: "ci"
      environment: 'PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.12"'
      jdk: 8
      group: 1
    secrets:
      s3_bucket: ${{ secrets.s3_bucket }}
      s3_access_key: ${{ secrets.s3_access_key }}
      s3_secret_key: ${{ secrets.s3_secret_key }}
      glue_schema_access_key: ${{ secrets.glue_schema_access_key }}
      glue_schema_secret_key: ${{ secrets.glue_schema_secret_key }}

  docs-404-check:
    runs-on: ubuntu-latest
    steps:
      - name: "Checks out Flink"
        uses: actions/checkout@v2

      - name: "Check if PR contains docs change"
        run: |
          source ./tools/azure-pipelines/build_properties.sh
          pr_contains_docs_changes

      - name: "Builds docs"
        run: ./tools/ci/docs.sh
